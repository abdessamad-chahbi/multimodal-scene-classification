# ğŸ–¼ï¸ Multimodal Scene Classification

> ğŸ§  Multimodal scene classification combining **visual** (CNNs: VGG19, ResNet50, InceptionV3) and **audio** (MFCC â†’ LSTM) features.
> The project investigates **early, late and hybrid fusion strategies** and uses ensemble methods (bagging, voting) to boost performance â€” reaching **up to ~99% accuracy** with a hybrid/ensemble pipeline on the chosen dataset.

---

## ğŸ“˜ Table of Contents

- Project overview  
- Objectives  
- Dataset  
- Methodology  
- Fusion strategies
- Experimental Results
- Evaluation Metrics  
- Tech Stack 
- Key Skills & Concepts  
- Results Summary  
- Conclusion  
- Future work
- Useful Links 
- Author

---

## ğŸ“˜ Project overview

This project focuses on **multimodal scene classification**, combining **visual** and **audio** data to recognize environmental contexts.
The system integrates **image features** (from CNNs like VGG19, ResNet50, and InceptionV3) and **audio features** (MFCC processed with LSTM) using **fusion-based architectures** â€” early, late, and hybrid.

Through these approaches, we achieved **up to 99% accuracy** on the **Kaggle Scene Classification Dataset**, demonstrating the effectiveness of combining complementary modalities.

---

## ğŸ¯ Objectives

* **Visual Feature Extraction:** Use pre-trained CNNs (VGG19, ResNet50, InceptionV3) on ImageNet.
* **Audio Feature Extraction:** Derive MFCCs and learn temporal dependencies using LSTM networks.
* **Multimodal Fusion:** Explore early, late, and hybrid fusion architectures.
* **Performance Evaluation:** Compare models via Accuracy, Precision, Recall, F1-score, and Confusion Matrices.
* **Ensemble Learning:** Implement bagging and majority voting to improve robustness.

---

## ğŸ§© Dataset

This project uses the **Kaggle â€œScene Classification: Images and Audioâ€** dataset, which includes **images** paired with **MFCC audio features**.

ğŸ“Œ Dataset link:
[https://www.kaggle.com/code/kerneler/starter-scene-classification-images-40e223fe-3/](https://www.kaggle.com/code/kerneler/starter-scene-classification-images-40e223fe-3/)

### ğŸ“Š Dataset Summary

* **17,252 total samples**
* Each sample contains:

  * âœ… One environment **image**
  * âœ… Associated **audio MFCC features**
* **9 scene classes**, such as:
  *Beach, City, Classroom, Restaurant, Forest, â€¦*

### ğŸ“ Data Structure

```
dataset/
â”‚
â”œâ”€â”€ images/                  # All scene images grouped by class
â”‚   â”œâ”€â”€ beach/
â”‚   â”œâ”€â”€ forest/
â”‚   â”œâ”€â”€ restaurant/
â”‚   â””â”€â”€ ...
â”‚
â””â”€â”€ dataset.csv              # image paths + MFCC features + class labels
      â”œâ”€â”€ image paths
      â”œâ”€â”€ mfcc_1 ... mfcc_104
      â”œâ”€â”€ CLASS1 (main label)
      â””â”€â”€ CLASS2 (detailed label)
```

ğŸ” Each row in the CSV links an (image â†’ MFCC audio â†’ scene category), enabling **multimodal learning**.

---

## âš™ï¸ Methodology

### 1ï¸âƒ£ Data Preprocessing

* **Images:**

  * Resized and normalized.
  * Denoising (median filter) and contrast enhancement.
* **Audio:**

  * Loaded from CSV, normalized, label-encoded, and aligned with image data.

### 2ï¸âƒ£ Feature Extraction

* **Visual:** CNN-based embeddings from VGG19, ResNet50, InceptionV3 (fc2 layer).
* **Audio:** Temporal features using MFCC + LSTM layers.

### 3ï¸âƒ£ Model Architectures

#### ğŸŒ€ Late Fusion

* Independent CNN and LSTM training.
* Fusion by:

  * **Average** of probabilities.
  * **Bagging** (Random Forest) on predicted outputs.

#### âš¡ Early Fusion

* Concatenation of image and audio features.
* Classifiers: **Dense NN**, **SVM (RBF kernel)**, and **Random Forest**.

#### ğŸš€ Hybrid Fusion

* Intermediate fusion of feature representations.
* Combines benefits of early (joint feature learning) and late (robust decision) fusions.

---

## ğŸ” Fusion strategies

- Early (feature-level) fusion: concatenate embeddings before classification. Good for models that can learn joint representations.
  - Concatenate visual + audio embeddings and train classifiers (NN / SVM / RF). 
- Late (decision-level) fusion: combine model outputs (probabilities) â€” simple averaging or stacked/ensemble classifier. Useful when modalities have independent strengths.
  - train image and audio models independently and combine their predictions (average, voting, or bagging features into a meta-classifier). 
- Hybrid fusion: mix early and late fusion to capture cross-modal interactions and model-specific strengths; often yields the best trade-off.
  - split training set to train unimodal models and a combined model; average or stack predictions for final decision.

---

## ğŸ§  Experimental Results

| **Fusion Type** | **Best Model**   | **Feature Extractor**   | **Accuracy** | **Notes**                                  |
| --------------- | ---------------- | ----------------------- | ------------ | ------------------------------------------ |
| Late Fusion     | Average Ensemble | VGG19 + LSTM            | **0.98**     | Consistent across all classes              |
| Late Fusion     | Bagging (RF)     | VGG19 + LSTM            | 0.97         | Slightly lower, strong recall              |
| Early Fusion    | Dense NN         | VGG19 + MFCC-LSTM       | 0.96         | Great balance between precision and recall |
| Early Fusion    | SVM              | VGG19 + MFCC-LSTM       | 0.96         | Best for â€œRestaurantâ€ class                |
| Early Fusion    | Random Forest    | VGG19 + MFCC-LSTM       | 0.95         | Stable across environments                 |
| Hybrid Fusion   | Dense NN         | InceptionV3 + MFCC-LSTM | **0.99**     | Top performer â€” best multimodal synergy    |

âœ… **Highest accuracy (99%)** obtained with **InceptionV3 + LSTM (Hybrid Fusion)**.
**Confusion reduction** achieved by combining complementary modalities (visual + temporal).

---

## ğŸ“Š Evaluation Metrics

* **Accuracy (overall correctness)**
* **Precision & Recall (per class performance)**
* **F1-Score (harmonic mean of precision/recall)**
* **Confusion Matrix (error analysis)**

All metrics confirm the **superiority of hybrid fusion**, particularly for visually similar categories (e.g., *Forest vs Jungle*).

---

## ğŸ§° Tech Stack

* **Languages & Libraries:**

  * ğŸ Python, NumPy, Pandas, Matplotlib
  * ğŸ›ï¸ TensorFlow, Keras, Scikit-learn
  * ğŸ§© OpenCV, Librosa (for MFCC)
* **Models:**

  * CNN (VGG19, ResNet50, InceptionV3)
  * LSTM (Audio temporal features)
  * Dense Neural Networks, SVM, Random Forest
* **Tools:** Jupyter Notebook, Kaggle, Google Colab

---

## ğŸš€ Key Skills & Concepts

* Transfer Learning (VGG19, InceptionV3)
* Convolutional Neural Networks (CNN)
* Recurrent Neural Networks (RNN, LSTM)
* MFCC Feature Extraction
* Early, Late & Hybrid Multimodal Fusion
* Ensemble Learning (Bagging, Voting)
* Model Evaluation & Visualization

---

## ğŸ“ˆ Results Summary

> â€œMultimodal learning leads to richer feature representation â€” achieving **human-like perception** through complementary cues.â€

| Model                       | Accuracy | Precision | Recall   | F1-score |
| --------------------------- | -------- | --------- | -------- | -------- |
| CNN (VGG19)                 | 0.95     | 0.94      | 0.95     | 0.95     |
| LSTM (Audio)                | 0.94     | 0.94      | 0.94     | 0.94     |
| Hybrid (InceptionV3 + LSTM) | **0.99** | **0.99**  | **0.99** | **0.99** |

---

## ğŸ Conclusion

This project demonstrates that **combining visual and auditory modalities** significantly improves scene classification accuracy compared to unimodal approaches.
The **hybrid fusion strategy** effectively leverages both spatial and temporal cues, proving the potential of multimodal deep learning for complex perception tasks.

---

## ğŸ¬ Demo

> â–¶ï¸ See the system classify a scene using **both image and audio cues**  
> ğŸŒ **Multimodal Scene Classification â€“ Demo Video**  
> ğŸ”— [https://github.com/abdessamad-chahbi/multimodal-scene-classification/blob/main/demo.mp4](https://github.com/abdessamad-chahbi/multimodal-scene-classification/blob/main/demo.mp4)  

---

## ğŸ”® Future work

- Explore attention-based multimodal fusion (cross-modal transformers).  
- Fine-tune visual backbones end-to-end with multi-task losses.  
- Add temporal modeling across frames (3D CNNs / temporal attention) for video sequences.  
- Deploy a small inference API for real-time multimodal classification.

---

## ğŸ”— Useful Links

* ğŸ§¾ Dataset: [Scene Classification (Images & Audio) â€“ Kaggle](https://www.kaggle.com/code/kerneler/starter-scene-classification-images-40e223fe-3/)
* ğŸ’» Project Repository: [GitHub â€“ abdessamad-chahbi/multimodal-scene-classification](https://github.com/abdessamad-chahbi/multimodal-scene-classification)

---

## ğŸ‘¤ Author

**Abdessamad CHAHBI**
â€¢ ğŸŒ [Portfolio](https://abdessamad-chahbi.github.io)
â€¢ [LinkedIn](https://linkedin.com/in/abdessamad-chahbi) 
â€¢ [GitHub](https://github.com/abdessamad-chahbi)

---
